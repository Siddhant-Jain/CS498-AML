---
title: "AML hw5"
author: "Roshan Rajan, Kirsten Wong, Kenneth Zhang"
date: "March 11, 2017"
output: html_document
---

## Q1:
The UCI Machine Learning dataset repository hosts a dataset giving features of music, and the latitude and longitude from which that music originates.
```{r, echo=TRUE}
library(MASS)
library(glmnet)
tracks = read.csv("default_plus_chromatic_features_1059_tracks.txt", header=FALSE)
```
First, build a straightforward linear regression of latitude (resp. longitude) against features. What is the R-squared? Plot a graph evaluating each regression.
```{r, echo=TRUE}
latfit = lm(tracks[,117] ~ rowSums(tracks[,1:116]))
longfit = lm(tracks[,118] ~ rowSums(tracks[,1:116]))

plot(latfit, which=c(1,1))
plot(longfit, which=c(1,1))

print(c(summary(latfit)$r.squared, summary(longfit)$r.squared))
```
Does a Box-Cox transformation improve the regressions? Notice that the dependent variable has some negative values, which Box-Cox doesn't like. You can deal with this by remembering that these are angles, so you get to choose the origin. why do you say so? For the rest of the exercise, use the transformation if it does improve things, otherwise, use the raw data.
```{r, echo=TRUE}
latfit2 = lm(tracks[,117] - min(tracks[,117])+1 ~ rowSums(tracks[,1:116]+1))
longfit2 = lm(tracks[,118] - min(tracks[,118])+1 ~ rowSums(tracks[,1:116]+1))
plot(rowSums(tracks[,1:116]), tracks[,117]+100)

bclat = boxcox(latfit2, lambda=seq(-6,6,0.1))
bclong = boxcox(longfit2, lambda=seq(-6,6,0.1))

#finds best lambda from each boxcox
lambdalat = bclat$x[which.max(bclat$y)]
lambdalong = bclong$x[which.max(bclong$y)]

#transforms the dependent variable based on boxcox lambda
latfit2 = lm((tracks[,117]^lambdalat-1)/lambdalat ~ rowSums(tracks[,1:116]))
longfit2 = lm((tracks[,118]^lambdalong-1)/lambdalong ~ rowSums(tracks[,1:116]))
print(c(summary(latfit2)$r.squared, summary(longfit2)$r.squared))
```
Use glmnet to produce:
A regression regularized by L2 (equivalently, a ridge regression). You should estimate the regularization coefficient that produces the minimum error. Is the regularized regression better than the unregularized regression?
```{r, echo=TRUE}
#lasso: alpha=1
#ridge: alpha=0
#elastic: alpha=0.5
ridgefit = cv.glmnet(as.matrix(tracks[,1:116]), as.matrix(tracks[,117]), alpha=0)
plot(ridgefit)

newy = predict(ridgefit, newx = as.matrix(tracks[,1:116]), s = "lambda.min")
ridgefit2 = lm(newy ~ rowSums(tracks[,1:116]))
print(summary(ridgefit2)$r.squared)
```
A regression regularized by L1 (equivalently, a lasso regression). You should estimate the regularization coefficient that produces the minimum error. How many variables are used by this regression? Is the regularized regression better than the unregularized regression?

```{r, echo=TRUE}
lassofit = cv.glmnet(as.matrix(tracks[,1:116]), as.matrix(tracks[,117]), alpha=1)
plot(lassofit)
```

## Q2:
The UCI Machine Learning dataset repository hosts a dataset giving whether a Taiwanese credit card user defaults against a variety of features.
```{r, echo=TRUE}
card_data = read.csv("card.csv", header=TRUE)
```
Use logistic regression to predict whether the user defaults. You should ignore outliers, but you should try the various regularization schemes we have discussed.
```{r, echo=TRUE}

```

## Q3:
A wide dataset, from cancer genetics: In "Broad patterns of gene expression revealed by clustering of tumor and normal colon tissues probed by oligonucleotide arrays" by U. Alon, N. Barkai, D. A. Notterman, K. Gish, S. Ybarra, D. Mack, and A. J. Levine, Proc. Natl. Acad. Sci. USA, Vol. 96, Issue 12, 6745-6750, June 8, 1999, authors collected data giving gene expressions for tumorous and normal colon tissues. You will find this dataset here. There is a matrix of gene expression levels for 2000 genes (these are the independent variables) for 62 tissue samples. As you can see, there are a lot more independent variables than there are data items. At that website, you will also find a file giving which sample is tumorous and which is normal.
```{r, echo=TRUE}
gene_data = read.csv("genes_header.txt", header=TRUE)
```
Use a binomial regression model (i.e. logistic regression) with the lasso to predict tumorous/normal. Use cross-validation to assess how accurate your model is. Report both AUC (below) and deviance. How many genes does the best model use?
```{r, echo=TRUE}

```

## Q4:
AUC: is one standard measure of classification performance, reported by glmnet; look this up here , but the key phrase is "When using normalized units, the area under the curve (often referred to as simply the AUC, or AUROC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative')."
```{r, echo=TRUE}

```