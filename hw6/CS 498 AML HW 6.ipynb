{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 498 AML HW 6\n",
    "\n",
    "#### Roshan Rajan (rjrajan2), Kirsten Wong (kewong2), Kenneth Zhang (kfzhang2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Machine Learning dataset repository hosts several datasets recording word counts for documents here. You will use the NIPS dataset.<br><br> You will find:<br> (a) a table of word counts per document and <br>\n",
    "(b) a vocabulary list for this dataset at the link. You must implement the multinomial mixture of topics model, lectured in class.<br><br>\n",
    "For this problem, you should write the clustering code yourself (i.e. not use a package for clustering).\n",
    "Cluster this to 30 topics, using a simple mixture of multinomial topic model, as lectured in class.\n",
    "Produce a graph showing, for each topic, the probability with which the topic is selected.\n",
    "Produce a table showing, for each topic, the 10 words with the highest probability for that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read files.\n",
    "# docword stores the document word counts, vocab stores the vocabulary list.\n",
    "import os\n",
    "dirname = os.getcwd() + \"/docword.nips.noheader.txt\"\n",
    "docword = np.loadtxt(dirname)\n",
    "dirname = os.getcwd() + \"/vocab.nips.txt\"\n",
    "with open(dirname) as f:\n",
    "    vocab = f.readlines()\n",
    "vocab = [x.strip() for x in vocab] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform data into a 1500 x 12419 matrix where each doc is a data point, and each column i is a value\n",
    "# indicating the frequency of word number i. Matrix should be mostly empty.\n",
    "docdata = np.zeros((1501,12420))\n",
    "    \n",
    "for i in range(0,docword.shape[0]):\n",
    "    docdata[docword[i][0]][docword[i][1]] += docword[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare parameters\n",
    "n_topics = 30\n",
    "n_docs = 1500\n",
    "n_words = 12419\n",
    "n_nz = 746316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use k-means to initialize centers\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=30, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create k-means object and train it on dataset (~150 MB of data), took ~45 seconds\n",
    "km = KMeans(n_clusters = 30, random_state = 0)\n",
    "km.fit(docdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store initial guess for cluster centers.\n",
    "initial_clst = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EM algorithm. Input: dataset, initial cluster centers, computed above\n",
    "def EM(ds,init_clst):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation using EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can segment an image using a clustering method - each segment is the cluster center to which a pixel belongs. In this exercise, you will represent an image pixel by its r, g, and b values (so use color images!). Use the EM algorithm applied to the mixture of normal distribution model lectured in class to cluster image pixels, then segment the image by mapping each pixel to the cluster center with the highest value of the posterior probability for that pixel. You must implement the EM algorithm yourself (rather than using a package).<br><br>\n",
    "Segment each of the test images to 10, 20, and 50 segments. You should display these segmented images as images, where each pixel's color is replaced with the mean color of the closest segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read files.\n",
    "\n",
    "dirname = os.getcwd() + \"/img1.jpg\"\n",
    "img1 = scipy.misc.imread(dirname)\n",
    "dirname = os.getcwd() + \"/img2.jpg\"\n",
    "img2 = scipy.misc.imread(dirname)\n",
    "dirname = os.getcwd() + \"/img3.jpg\"\n",
    "img3 = scipy.misc.imread(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusts1 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusts2 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusts3 = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will identify one special test image. You should segment this to 20 segments using five different start points, and display the result for each case. Is there much variation in the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
